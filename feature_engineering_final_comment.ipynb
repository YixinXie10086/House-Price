{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_list():# create list of data type based on the descrption \n",
    "    categ=['MSSubClass','MSZoning','Street','Alley',\"LotShape\",\"LandContour\",\"Utilities\",\"LotConfig\",\\\n",
    "      \"LandSlope\",\"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\"OverallQual\",\\\n",
    "      \"OverallCond\",'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual',\\\n",
    "      'ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\\\n",
    "      \"Heating\",\"HeatingQC\",\"Electrical\",'KitchenQual','Functional','FireplaceQu','GarageType',\\\n",
    "      'GarageFinish','GarageCars','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature',\\\n",
    "       'SaleType','SaleCondition','CentralAir']\n",
    "    # note categorical include binary and categ_usefulNA \n",
    "\n",
    "\n",
    "       \n",
    "    binary=['HeatingQC','CentralAir'] # binary categorical\n",
    "    categ_usefulNA=[\"Alley\",'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType',\\\n",
    "                     'GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\n",
    "    #categorical that have NA with useful meaning(instead of missing)\n",
    "   \n",
    "    numerical=['GrLivArea','LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\\\n",
    "          'GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea',\\\n",
    "          'MiscVal','1stFlrSF','2ndFlrSF','LowQualFinSF']\n",
    "   \n",
    "    #note that numerical do not have year or int_data\n",
    "    Year=['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']\n",
    "\n",
    "    int_data=['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\\\n",
    "          'TotRmsAbvGrd',\"Fireplaces\",'MoSold']\n",
    "\n",
    "  \n",
    "    #list of categorical variable, but NA is not missing data.\n",
    "    return categ, binary, numerical, Year, int_data, categ_usefulNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_data_type(data): # correct the datatype according to the data description\n",
    "    data[data.columns.intersection(dat_list()[0])]=data[data.columns.intersection(dat_list()[0])].astype('category')\n",
    "    # using the list from dat_list to find columns\n",
    "    data[data.columns.intersection(dat_list()[2])]=data[data.columns.intersection(dat_list()[2])].astype('float')\n",
    "    data[data.columns.intersection(dat_list()[3])]=data[data.columns.intersection(dat_list()[3])].astype('int',errors='ignore')#\n",
    "    data[data.columns.intersection(dat_list()[4])]=data[data.columns.intersection(dat_list()[4])].astype('int',errors='ignore')\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def replace_na(df):\n",
    "    df[dat_list()[5]]=df[df.columns.intersection(dat_list()[5])].replace(float('nan'),'Donthave')\n",
    "    # these are the variables that has useful Na\n",
    "    for column in dat_list()[0]: # replace categorical with mode()\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "    for column in dat_list()[2]: # replace float with median\n",
    "        df[column].fillna(df[column].median(), inplace=True)\n",
    "    for column in dat_list()[3]: # replace year with median \n",
    "        df[column].fillna(df[column].median(), inplace=True)\n",
    "    for column in dat_list()[4]: # replace integer value with median \n",
    "        df[column].fillna(df[column].median(), inplace=True)\n",
    "    train=df.dropna()\n",
    "    test=df[df[\"SalePrice\"].isna()!=False]\n",
    "    return train, test, df\n",
    "\n",
    "#complete_data=correct_data_type(complete_data)\n",
    "#complete_data=replace_na(complete_data)[2]\n",
    "#complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from numpy import array \n",
    "\n",
    "\n",
    "\n",
    "def constant_exclude(dataset,n):\n",
    "    Data=dataset                 #Import the complete set\n",
    "    numeric=pd.concat([Data[Data.columns&dat_list()[2]],Data[Data.columns&dat_list()[3]],Data[Data.columns&dat_list()[4]]],axis=1) # get only numeric variables\n",
    "    numeric_normal=normalize(numeric) #Normalize the values of numeric variables\n",
    "    sel = VarianceThreshold(threshold=n) #Determine the threshold\n",
    "    sel.fit(numeric_normal) #Variance lower than the threshold get removed.\n",
    "    value=sel.get_support() \n",
    "    reduced_numeric=numeric.loc[:,value] #Get remaining columns\n",
    "    complete_set=pd.concat([Data['SalePrice'],Data['Id'],reduced_numeric,Data[Data.columns&dat_list()[0]]],axis=1) #Add back the categorical columns, \"Id\" and \"Sale Price\".\n",
    "    return complete_set, sum(value)   #Return complete set, number of remaining columns.\n",
    "\n",
    "def k_best(data,n):\n",
    "    Data=data  #Import the complete set\n",
    "    train=data[data['SalePrice'].isna()==False] #get the traininng data\n",
    "    numeric=dat_list()[2]+dat_list()[3]+dat_list()[4] #Get the numeric variables.\n",
    "    numeric=train[train.columns.intersection(numeric)].copy() \n",
    "    New = SelectKBest(f_regression, k=n) #Select variables with highest kth F-statistic score\n",
    "    New.fit_transform(numeric, train['SalePrice']) \n",
    "    Filter = New.get_support()\n",
    "    Reduced_numeric=numeric.loc[:,Filter] #Get remaining columns\n",
    "    output=Data[Reduced_numeric.columns]\n",
    "    output['SalePrice']=Data['SalePrice'] #Add back Saleprice\n",
    "    output['Id']=Data['Id'] #Add back \"Id\"\n",
    "    categorical= dat_list()[0]\n",
    "    output[categorical]=Data[categorical] #Add back categorical variables.\n",
    "    return  output\n",
    "\n",
    "def cate_to_dummy(data):\n",
    "    Data=data\n",
    "    dummy_set=pd.get_dummies(Data[Data.columns.intersection(dat_list()[0])]) #Transform categorical data to dummy variables\n",
    "    total=pd.concat([Data['SalePrice'],Data['Id'],dummy_set,Data[Data.columns&dat_list()[2]],Data[Data.columns&dat_list()[3]],Data[Data.columns&dat_list()[4]]],axis=1) #Add back the numeric variables and \"Id\" and \"Sale Price\"\n",
    "    return total\n",
    "\n",
    "\n",
    "\n",
    "#constant_exclude(complete_data,0.002)[0]\n",
    "#complete_data=k_best(complete_data,12)\n",
    "#complete_data=cate_to_dummy(complete_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forward_select(data):\n",
    "    Data=data\n",
    "    lr = LinearRegression()\n",
    "    # After test the numbers of k_features = 40 satisfy our purpose\n",
    "    sfs = SFS(lr, k_features = 40, forward = True, floating = False, verbose = 2 #get details\n",
    "              ,scoring = 'r2', cv = 10 ) #neg_mean_squared_error to be consider\n",
    "    train = Data[0:1460]\n",
    "    test = Data[1460:2920]\n",
    "    y_train = train['SalePrice'].to_numpy()\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    X_train = train.drop('SalePrice',axis = 1)\n",
    "    X_test = test.drop('SalePrice',axis = 1)\n",
    "    sfs.fit(X_train,y_train)\n",
    "    x_variable = list(sfs.k_feature_names_)\n",
    "    X_train = X_train[x_variable]\n",
    "    lr.fit(X_train,y_train)\n",
    "    X_test = X_test[x_variable].to_numpy()\n",
    "    y_pred = lr.predict(X_test)\n",
    "    id = np.array([range(1461,2920)])\n",
    "    Id = pd.DataFrame(id.reshape(-1,1), columns = ['Id'])\n",
    "    saleprice = pd.DataFrame(y_pred, columns = ['SalePrice'])\n",
    "    total = pd.concat([Id,saleprice],axis = 1)\n",
    "\n",
    "    return Data[x_variable], total\n",
    "\n",
    "def stepwise_select(data):\n",
    "    Data=data\n",
    "    lr = LinearRegression()\n",
    "    # set up the train\n",
    "    train = Data[0:1460]\n",
    "    test = Data[1460:2920]\n",
    "    y_train = train['SalePrice'].to_numpy()\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    X_train = train.drop('SalePrice',axis = 1)\n",
    "    X_test = test.drop('SalePrice',axis = 1)\n",
    "\n",
    "    # After test the numbers of k_features = 40 satisfy our purpose\n",
    "    sffs = SFS(LinearRegression(), k_features = (1,40), forward = True, floating = True,verbose = 2,\n",
    "              scoring = 'r2', cv = 10 )\n",
    "    sffs.fit(X_train,y_train)\n",
    "    x_variable = list(sffs.k_feature_names_)\n",
    "    # updated the selected features to do linear regression prediction\n",
    "    X_train = X_train[x_variable]\n",
    "    \n",
    "    lr.fit(X_train,y_train)\n",
    "    X_test = X_test[x_variable]\n",
    "    y_pred = lr.predict(X_test)\n",
    "    id = np.array([range(1461,2920)])\n",
    "    Id = pd.DataFrame(id.reshape(-1,1), columns = ['Id'])\n",
    "    saleprice = pd.DataFrame(y_pred, columns = ['SalePrice'])\n",
    "    total = pd.concat([Id,saleprice],axis = 1)\n",
    "    \n",
    "    return Data[x_variable], total\n",
    "\n",
    "def backward_select(data):\n",
    "    lr = LinearRegression()\n",
    "    Data=data\n",
    "    # After test the numbers of k_features = 40 satisfy our purpose\n",
    "    sbs = SFS(lr, k_features = 40, forward = False, floating = False,verbose = 2,\n",
    "              scoring = 'r2', cv = 10 )\n",
    "    # set up the train\n",
    "    train = Data[0:1460]\n",
    "    test = Data[1460:2920]\n",
    "    y_train = train['SalePrice'].to_numpy()\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    X_train = train.drop('SalePrice',axis = 1)\n",
    "    X_test = test.drop('SalePrice',axis = 1)\n",
    "    sbs.fit(X_train,y_train)\n",
    "    x_variable = list(sbs.k_feature_names_)\n",
    "    # Update X_train\n",
    "    X_train = X_train[x_variable]\n",
    "    X_test = X_test[x_variable]\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    id = np.array([range(1461,2920)])\n",
    "    Id = pd.DataFrame(id.reshape(-1,1), columns = ['Id'])\n",
    "    saleprice = pd.DataFrame(y_pred, columns = ['SalePrice'])\n",
    "    total = pd.concat([Id,saleprice],axis = 1)\n",
    "    return Data[x_variable], total\n",
    "\n",
    "#backward_select(complete_data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_xy(df):# you should input the complete dataset here.\n",
    "    \n",
    "    training=df[df['SalePrice'].isna()==False] # for training, the Saleprice should not be missing\n",
    "    training\n",
    "    testing=df[df['SalePrice'].isna()==True] # for testing, the Saleprice should be NA's, use this to track testing\n",
    "\n",
    "    y_train=training['SalePrice']\n",
    "    x_train= training.drop(['Id','SalePrice'],axis=1)\n",
    "    x_test= testing.drop(['SalePrice'],axis=1)\n",
    "    # for x_test, we want to keep the id for tracking the output id in the final stage \n",
    "    return x_train,y_train, x_test\n",
    "    \n",
    "\n",
    "\n",
    "def lass_CV(df,xtrain,ytrain,xtest):\n",
    "    Id=xtest['Id']\n",
    "    xtest= xtest.drop(['Id'],axis=1)\n",
    "    \n",
    "    numerical_dat= dat_list()[2]+dat_list()[3]+dat_list()[4] # find all numerical data\n",
    "    numerical_dat=xtrain.columns.intersection(numerical_dat) \n",
    "    # because we might delete some columns in filter methods, so we use intersection to find the columns still left.\n",
    "    scaler= StandardScaler() # scale the data first before fitting\n",
    "    xtrain[numerical_dat]= scaler.fit_transform(xtrain[numerical_dat])\n",
    "    xtest[numerical_dat] = scaler.fit_transform(xtest[numerical_dat])\n",
    "    model = lm.LassoCV(cv=10, random_state=0, max_iter=10000) # use cross validation score to find penalty term\n",
    "    model.fit(xtrain, ytrain)\n",
    "    model.alpha_\n",
    "    lasso_best = lm.Lasso(alpha=model.alpha_) # fit best model with best alpha\n",
    "    lasso_best.fit(xtrain, ytrain)\n",
    "    result=lasso_best.predict(xtest)\n",
    "    output=pd.DataFrame()\n",
    "    output['Id']= Id\n",
    "    output['Saleprice']=result # Kaggle requires the result to have 2 columns: Id and Saleprice\n",
    "    return output  \n",
    "\n",
    "def Rid_CV(df,xtrain,ytrain,xtest):\n",
    "    # ridge is very similar to lasso\n",
    "    Id=xtest['Id']\n",
    "    xtest= xtest.drop(['Id'],axis=1)\n",
    "    \n",
    "    numerical_dat= dat_list()[2]+dat_list()[3]+dat_list()[4]\n",
    "    numerical_dat=xtrain.columns.intersection(numerical_dat)\n",
    "    scaler= StandardScaler()\n",
    "    xtrain[numerical_dat]= scaler.fit_transform(xtrain[numerical_dat])\n",
    "    xtest[numerical_dat] = scaler.fit_transform(xtest[numerical_dat])\n",
    "    model = lm.RidgeCV(cv=10)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ridge_best = lm.Ridge(alpha=model.alpha_)\n",
    "    ridge_best.fit(xtrain, ytrain)\n",
    "    result=ridge_best.predict(xtest)\n",
    "    output=pd.DataFrame()\n",
    "    output['Id']= Id\n",
    "    output['Saleprice']=result\n",
    "    return output\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "def random_forest_default(df,xtrain,ytrain,xtest):\n",
    "        Id=xtest['Id']\n",
    "        xtest= xtest.drop(['Id'],axis=1)\n",
    "        rf = RandomForestRegressor() # call the default model \n",
    "        rf.fit(xtrain,ytrain)\n",
    "        result= rf.predict(xtest) # use default model to train and predict \n",
    "        output=pd.DataFrame()\n",
    "        output['Id']= Id\n",
    "        output['Saleprice']=result\n",
    "        return output\n",
    "             \n",
    "def rf_cv_tun():# this function does a automatic tunning for random forest\n",
    "    rf=RandomForestRegressor()\n",
    "    n_estimators = [1000,2000,3000]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    min_samples_split = [x/10 for x in np.linspace(2, 10, num = 5)]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap} # input values into the parameter space\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5,random_state=30, n_jobs = -1)\n",
    "    # use random search and cross validation to find best combinations in parameter space. \n",
    "    return rf_random\n",
    "\n",
    "\n",
    "def random_forest_best(df,xtrain,ytrain,xtest,threshold,model):\n",
    "    #threshold is the feature score for which you want the features to be selected.\n",
    "    # use model from rf_cv_tun(), a model that tuns parameters with CV\n",
    "    \n",
    "    Id=xtest['Id']\n",
    "    xtest= xtest.drop(['Id'],axis=1)\n",
    "    \n",
    "    model.fit(xtrain,ytrain)\n",
    "    feature_scores = pd.Series(model.best_estimator_.feature_importances_, index=xtrain.columns).sort_values(ascending=False)\n",
    "    #find the feature score of the model\n",
    "    important_feature=feature_scores.index.where(feature_scores>threshold).dropna()\n",
    "    # filer variables that has feature importance above threshold\n",
    "    # select those variables in x train and xtest\n",
    "    xtrain=xtrain[important_feature]\n",
    "    xtest=xtest[important_feature]\n",
    "    model.fit(xtrain,ytrain)\n",
    "    result=model.predict(xtest)\n",
    "    output=pd.DataFrame()\n",
    "    output['Id']= Id\n",
    "    output['Saleprice']=result\n",
    "    return output\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "def rand_cv_tunning(): # a function that tuns the XGB model\n",
    "    parameters = {'objective':['reg:squarederror'],\n",
    "                  'booster':['gbtree'], # can be gbtree or linear, but generally trees have better performance\n",
    "                  'learning_rate': [x for x in np.linspace(start = 0.05, stop = 0.3, num = 6)], \n",
    "                  # a number between 0 to 1, determines the learning rate\n",
    "                  'max_depth': [5,10,15,20],\n",
    "                  'min_child_weight': [10,15,20,25],\n",
    "                 'colsample_bytree': [i/10 for i in range(1,10)],\n",
    "                  'n_estimators': [1000,2000]}\n",
    "\n",
    "    xgb_model = XGBRegressor(random_state=30)\n",
    "    xgb = RandomizedSearchCV(xgb_model, parameters, cv=5,n_iter=100,scoring='neg_mean_squared_error',n_jobs=-1,random_state=30)\n",
    "    return xgb\n",
    "\n",
    "#xgb_model=rand_cv_tunning()\n",
    "\n",
    "\n",
    "\n",
    "def xgb_default(complete_data,xtrain,ytrain,xtest):\n",
    "    Id=xtest['Id']\n",
    "    xtest= xtest.drop(['Id'],axis=1)\n",
    "    xgb_model = XGBRegressor(random_state=30)\n",
    "    xgb_model.fit(xtrain,ytrain)\n",
    "    result= xgb_model.predict(xtest)\n",
    "    \n",
    "    output=pd.DataFrame()\n",
    "    output['Id']= Id\n",
    "    output['Saleprice']=result\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def XGB_with_tun(df,xtrain, ytrain, xtest,model):# use model from rand_cv_tunning()\n",
    "       Id=xtest['Id']\n",
    "        xtest= xtest.drop(['Id'],axis=1)\n",
    "        \n",
    "        model.fit(xtrain,ytrain)\n",
    "        result=model.predict(xtest)\n",
    "        output=pd.DataFrame()\n",
    "        output['Id']= Id\n",
    "        output['Saleprice']=result\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code\n",
    "wd=os.getcwd()\n",
    "\n",
    "path=wd+'/house_price data/train.csv'\n",
    "Path=wd+'/house_price data/test.csv'\n",
    "\n",
    "train_data=pd.read_csv(path)\n",
    "test_data=pd.read_csv(Path)\n",
    "train_data= pd.read_csv('../142A final project/train.csv')\n",
    "train_data=pd.DataFrame(train_data)\n",
    "\n",
    "test_data=pd.read_csv('../142A final project/test.csv')\n",
    "test_data=pd.DataFrame(test_data)\n",
    "\n",
    "#complete_data= pd.concat([train_data,test_data])\n",
    "#complete_data=correct_data_type(complete_data)\n",
    "#complete_data=replace_na(complete_data)[2]\n",
    "#complete_data\n",
    "#wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail 1\n",
    "# It goes through all wrapper methods and embedded methods with data filtered from filter methods \n",
    "data_1=complete_data\n",
    "data_1=k_best(data_1,12)\n",
    "data_1=constant_exclude(data_1,0.002)[0]\n",
    "data_1=cate_to_dummy(data_1)\n",
    "\n",
    "\n",
    "output1=forward_select(data_1)[1]\n",
    "output1.to_csv('/testing_1.csv', index = False) # output the result to CSV file\n",
    "\n",
    "output2=backward_select(data_1)[1]\n",
    "output2.to_csv('/testing_2.csv', index = False)\n",
    "\n",
    "output3=stepwise_select(data_1)[1]\n",
    "output3.to_csv('/testing_3.csv', index = False)\n",
    "\n",
    "\n",
    "x_train, y_train, x_test= generate_xy(data_1)\n",
    "output4=lass_CV(data_1,x_train,y_train,x_test)\n",
    "output4.to_csv('/testing_4.csv', index = False)\n",
    "\n",
    "output5=Rid_CV(data_1,x_train,y_train,x_test)\n",
    "output5.to_csv('/testing_5.csv', index = False)\n",
    "\n",
    "\n",
    "output6=random_forest_default(data_1,x_train,y_train,x_test)\n",
    "output6.to_csv('/testing_6.csv', index = False)\n",
    "\n",
    "\n",
    "output7=xgb_default(data_1,x_train,y_train,x_test)\n",
    "output7.to_csv('/testing_7.csv', index = False)\n",
    "\n",
    "rf_model= rf_cv_tun()\n",
    "output8=random_forest_best(data_1,x_train,y_train,x_test,0.0001,rf_model)\n",
    "output8.to_csv('/testing_8.csv', index = False)\n",
    "\n",
    "xgb_model=rand_cv_tunning()\n",
    "output9=XGB_with_tun(data_1,x_train,y_train,x_test,xgb_model)\n",
    "output9.to_csv('/testing_9.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=complete_data # this trail goes through the wrapper and embbeded methods without filter method\n",
    "\n",
    "data_2=cate_to_dummy(data_2)\n",
    "\n",
    "\n",
    "output10=forward_select(data_2)[1]\n",
    "output10.to_csv('/Users/lizehao727/Downloads/testing_10.csv', index = False)\n",
    "\n",
    "output11=backward_select(data_2)[1]\n",
    "output11.to_csv('/Users/lizehao727/Downloads/testing_11.csv', index = False)\n",
    "\n",
    "output12=stepwise_select(data_2)[1]\n",
    "output12.to_csv('/Users/lizehao727/Downloads/testing_12.csv', index = False)\n",
    "\n",
    "\n",
    "x_train, y_train, x_test= generate_xy(data_2)\n",
    "\n",
    "output13=lass_CV(data_2,x_train,y_train,x_test)\n",
    "output13.to_csv('/Users/lizehao727/Downloads/testing_13.csv', index = False)\n",
    "\n",
    "output14=Rid_CV(data_2,x_train,y_train,x_test)\n",
    "output14.to_csv('/Users/lizehao727/Downloads/testing_14.csv', index = False)\n",
    "\n",
    "\n",
    "output15=random_forest_default(data_2,x_train,y_train,x_test)\n",
    "output15.to_csv('/testing_15.csv', index = False)\n",
    "\n",
    "\n",
    "output16=xgb_default(data_2,x_train,y_train,x_test)\n",
    "output16.to_csv('/testing_16.csv', index = False)\n",
    "\n",
    "rf_model= rf_cv_tun()\n",
    "output17=random_forest_best(data_2,x_train,y_train,x_test,0.0001,rf_model)\n",
    "output17.to_csv('/testing_17.csv', index = False)\n",
    "\n",
    "xgb_model=rand_cv_tunning()\n",
    "output18=XGB_with_tun(data_2,x_train,y_train,x_test,xgb_model)\n",
    "output18.to_csv('/testing_18.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are plotting codes. Adjustment for variable name is required.\n",
    "\n",
    "#Variance bar plot:\n",
    "#data=remove_na(complete_data)[2]\n",
    "#Data=replace_na(data)[2]\n",
    "#numeric=pd.concat([Data[Data.columns&dat_list()[2]],Data[Data.columns&dat_list()[3]],Data[Data.columns&dat_list()[4]]],axis=1) #numeric variables\n",
    "#numeric_normal=normalize(numeric)\n",
    "#numeric_normal=pd.DataFrame(numeric_normal, columns=numeric.columns)\n",
    "#var=np.array(numeric_normal.var()).reshape(1,32)\n",
    "#Var=pd.DataFrame(var,columns=numeric.columns,index=['Variance_for_each_feature'])\n",
    "#Var=Var.sort_values(by='Variance_for_each_feature',axis=1,ascending=False)\n",
    "#Var.plot.bar(rot=0)\n",
    "#numeric_normal.var().sort_values(ascending=False)\n",
    "\n",
    "#F-statistic score bar plot:\n",
    "#Data=replace_na(trainset)[2]\n",
    "#numeric=pd.concat([Data[Data.columns&dat_list()[2]],Data[Data.columns&dat_list()[3]],Data[Data.columns&dat_list()[4]]],axis=1)\n",
    "#vvalue=f_regression(numeric,trainset['SalePrice'])\n",
    "#f_stat=pd.DataFrame(vvalue, columns=numeric.columns,index=['F_statistic','p_value'])\n",
    "#f_stat=f_stat.drop(labels='p_value',axis=0)\n",
    "#F_stat=f_stat.sort_values(by='F_statistic',axis=1,ascending=False)\n",
    "#F_stat['KitchenAbvGr']\n",
    "#F_stat.plot.bar(rot=0)\n",
    "#sum(F_stat.iloc[0]>27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
